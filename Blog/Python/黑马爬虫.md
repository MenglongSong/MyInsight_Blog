### 黑马爬虫学习

#### 1. 爬虫课程概述
1. Python的基础语法知识(待补充)  

2. 如何抓取HTML页面  
    HTTP请求的处理：urllib、urllib2、requests  
    处理后的请求可以模拟浏览器发送请求，获取服务器的响应  

3. 解析服务器响应的内容
    re、xpath、BeautifulSoup4(BS4)、jsonpath、pyquery等   
    使用某种描述性语言来给我们需要的数据定义一个规则,  
    符合这个规则的数据会被匹配  
    
4. 如何采集动态HTML、验证码的处理  
    通用的动态页面采集:Selenium + PhantomJS(无页面):模拟真实浏览器加载js、ajax等非静态页面数据  
    Tesseract:机器学习库，图像识别系统  
    
5. Scrapy框架:(Scrapy,Pyspider)  
    高定制性  
    高性能(异步网络框架Twisted)，所以数据下载速度非常快  
    提供了数据存储，数据下载，提取规则等组件  
    
6. 分布式策略:  
    scrapy-redis  
    在Scrapy的基础上添加了一套以Redis数据库为核心的一套组件，让Scrapy框架支持分布式功能  
    主要在Redis里做请求指纹去重，请求分配，数据临时存储  

7. 爬虫--反爬虫--反反爬虫 之间的斗争:  
    爬虫最到最后，最头疼的不是复杂的页面，也不是晦涩的数据，而是网站的反爬虫工程师  
    
    User-Agent、代理、验证码、动态数据加载、加密数据。  
    
    数据的价值是否值得去费劲做爬虫。  
    
    机器成本 + 人力成本 > 数据价值，就不反了，一般做到封IP就结束了。  
    
    
#### 2. 通用爬虫和聚焦爬虫介绍
通用爬虫:搜索引擎用的爬虫系统  

聚焦爬虫:爬虫程序员写的针对某种内容爬虫  

#### 3. HTTP和HTTPS复习学习以及Fiddle的使用
网络爬虫抓取数据的过程可以理解为 模拟浏览器操作的过程  

网络抓包工具:Fiddle  
Raw(总览)  

Cookie:通过在客户端记录的信息确定用户的身份  
Session:通过在服务器记录的信息确定用户的身份  

#### 4. URLLIB2
Python自带的模块: /usr/lib/python2.7/urllib2.py  
Python的第三方模块: /usr/local/lib/python2.7/site-packages  

urllib2 默认的 User-Agent: Python2.7-urllib/2.7    
(爬虫第一步需要重构User-Agent)  

response 是服务器响应的类文件，除了支持文件操作的方法，还支持以下常用的方法:  
服务器返回的类文件对象支持Python文件对象的操作方法  

read()方法就是读取文件里的全部内容，返回字符串  

返回 http的响应码 成功返回200 4XX服务器页面错误 5XX服务器问题  
print response2.getcode()  

返回实际数据的实际URL 防止重定向问题  
print response2.geturl()  

返回 服务器响应的http报头  
print response2.info()


#### 8. POST请求的模拟案例



#### 9. AJAX加载方式的数据获取
做爬虫最需要关注的不是页面信息 而是页面信息的来源  
AJAX方式加载的页面 数据来源一定是JSON  
拿到了JSON就是拿到网页的数据  


代理




